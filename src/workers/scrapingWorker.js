/**
 * Scraping Worker
 * Busca sites prontos para scraping e processa automaticamente
 */

import Site from '../models/Site.js';
import ScraperService from '../services/scraperService.js';

const ScrapingWorker = {
  /**
   * Executa scraping de sites prontos
   */
  async run() {
    console.log('\nüîç Worker de Scraping iniciado...');

    try {
      // Busca sites prontos para scraping
      const sites = await Site.findReadyToScrape();

      if (sites.length === 0) {
        console.log('   ‚ÑπÔ∏è Nenhum site pronto para scraping');
        return { processed: 0 };
      }

      console.log(`   üìã Sites para processar: ${sites.length}`);

      let totalArticles = 0;
      let processed = 0;

      for (const site of sites) {
        try {
          console.log(`\n   üåê ${site.name} (ID: ${site.id})`);

          const result = await ScraperService.scrapeSite(site.id);

          totalArticles += result.saved;
          processed++;

          console.log(`   ‚úÖ ${result.saved} novos artigos salvos`);

          // Delay entre sites para respeitar rate limit
          await new Promise(r => setTimeout(r, 2000));

        } catch (error) {
          console.error(`   ‚ùå Erro no scraping de ${site.name}:`, error.message);
        }
      }

      console.log(`\n   üéâ Scraping conclu√≠do: ${totalArticles} artigos de ${processed} sites`);

      return {
        processed,
        totalArticles
      };

    } catch (error) {
      console.error('   ‚ùå Erro no worker:', error.message);
      throw error;
    }
  }
};

export default ScrapingWorker;
